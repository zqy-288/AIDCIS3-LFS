"""
ç»Ÿè®¡åˆ†æå¼•æ“
è´Ÿè´£æ‰§è¡Œå„ç§ç»Ÿè®¡è®¡ç®—å’Œæ•°æ®åˆ†æ
"""

import logging
import threading
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

from PySide6.QtCore import QObject, Signal, QTimer
import numpy as np
import pandas as pd


class StatisticsEngine(QObject):
    """
    ç»Ÿè®¡åˆ†æå¼•æ“
    
    åŠŸèƒ½:
    1. æè¿°æ€§ç»Ÿè®¡åˆ†æ
    2. æ—¶é—´åºåˆ—åˆ†æ
    3. è´¨é‡è¶‹åŠ¿åˆ†æ
    4. å¼‚å¸¸æ£€æµ‹ç»Ÿè®¡
    5. å¤šç»´åº¦å¯¹æ¯”åˆ†æ
    """
    
    # ä¿¡å·å®šä¹‰
    analysis_started = Signal()
    analysis_progress = Signal(int, str)  # è¿›åº¦, çŠ¶æ€æè¿°
    analysis_completed = Signal(dict)     # åˆ†æç»“æœ
    analysis_failed = Signal(str)        # é”™è¯¯ä¿¡æ¯
    
    def __init__(self, data_model=None):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.data_model = data_model
        
        # åˆ†æçŠ¶æ€
        self.is_analyzing = False
        self.current_analysis = None
        
        # ç»Ÿè®¡é…ç½®
        self.confidence_level = 0.95
        self.outlier_threshold = 3.0  # æ ‡å‡†å·®å€æ•°
        
    def start_analysis(self, filters: Dict[str, Any]):
        """å¯åŠ¨ç»Ÿè®¡åˆ†æ"""
        if self.is_analyzing:
            self.logger.warning("âš ï¸ åˆ†ææ­£åœ¨è¿›è¡Œä¸­...")
            return
            
        try:
            self.logger.info("ğŸ”¬ å¯åŠ¨ç»Ÿè®¡åˆ†æ...")
            self.is_analyzing = True
            self.analysis_started.emit()
            
            # åœ¨åå°çº¿ç¨‹æ‰§è¡Œåˆ†æ
            analysis_thread = threading.Thread(
                target=self._execute_analysis,
                args=(filters,),
                daemon=True
            )
            analysis_thread.start()
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†æå¯åŠ¨å¤±è´¥: {e}")
            self.analysis_failed.emit(str(e))
            self.is_analyzing = False
            
    def _execute_analysis(self, filters: Dict[str, Any]):
        """æ‰§è¡Œåˆ†æè¿‡ç¨‹"""
        try:
            # æ­¥éª¤1: æ•°æ®å‡†å¤‡
            self.analysis_progress.emit(10, "å‡†å¤‡æ•°æ®...")
            raw_data = self._prepare_data(filters)
            
            # æ­¥éª¤2: åŸºç¡€ç»Ÿè®¡
            self.analysis_progress.emit(30, "è®¡ç®—åŸºç¡€ç»Ÿè®¡...")
            basic_stats = self._calculate_basic_statistics(raw_data)
            
            # æ­¥éª¤3: è´¨é‡åˆ†æ
            self.analysis_progress.emit(50, "åˆ†æè´¨é‡æŒ‡æ ‡...")
            quality_stats = self._analyze_quality_metrics(raw_data)
            
            # æ­¥éª¤4: è¶‹åŠ¿åˆ†æ
            self.analysis_progress.emit(70, "åˆ†æè¶‹åŠ¿...")
            trend_stats = self._analyze_trends(raw_data)
            
            # æ­¥éª¤5: å¼‚å¸¸æ£€æµ‹
            self.analysis_progress.emit(85, "æ£€æµ‹å¼‚å¸¸...")
            anomaly_stats = self._detect_anomalies(raw_data)
            
            # æ­¥éª¤6: æ•´åˆç»“æœ
            self.analysis_progress.emit(95, "æ•´åˆç»“æœ...")
            results = self._compile_results(
                basic_stats, quality_stats, trend_stats, anomaly_stats
            )
            
            # å®Œæˆ
            self.analysis_progress.emit(100, "åˆ†æå®Œæˆ")
            self.analysis_completed.emit(results)
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥: {e}")
            self.analysis_failed.emit(str(e))
        finally:
            self.is_analyzing = False
            
    def _prepare_data(self, filters: Dict[str, Any]) -> pd.DataFrame:
        """å‡†å¤‡åˆ†ææ•°æ®"""
        if not self.data_model:
            return pd.DataFrame()
            
        # ä»æ•°æ®æ¨¡å‹è·å–æ•°æ®
        raw_data = self.data_model.get_filtered_data(filters)
        
        # è½¬æ¢ä¸ºDataFrame
        if isinstance(raw_data, list):
            df = pd.DataFrame(raw_data)
        elif isinstance(raw_data, dict):
            df = pd.DataFrame([raw_data])
        else:
            df = raw_data
            
        # æ•°æ®æ¸…æ´—
        df = self._clean_data(df)
        
        return df
        
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        # ç§»é™¤ç©ºå€¼
        df = df.dropna(subset=['depth', 'diameter'])
        
        # æ•°æ®ç±»å‹è½¬æ¢
        if 'depth' in df.columns:
            df['depth'] = pd.to_numeric(df['depth'], errors='coerce')
        if 'diameter' in df.columns:
            df['diameter'] = pd.to_numeric(df['diameter'], errors='coerce')
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
            
        # ç§»é™¤å¼‚å¸¸å€¼
        df = self._remove_outliers(df)
        
        return df
        
    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        for column in ['depth', 'diameter']:
            if column in df.columns:
                mean = df[column].mean()
                std = df[column].std()
                threshold = self.outlier_threshold * std
                
                # ä¿ç•™åœ¨åˆç†èŒƒå›´å†…çš„æ•°æ®
                df = df[abs(df[column] - mean) <= threshold]
                
        return df
        
    def _calculate_basic_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_count': len(df),
            'depth_stats': {},
            'diameter_stats': {},
            'time_stats': {}
        }
        
        # æ·±åº¦ç»Ÿè®¡
        if 'depth' in df.columns and not df['depth'].empty:
            depth_series = df['depth']
            stats['depth_stats'] = {
                'mean': float(depth_series.mean()),
                'median': float(depth_series.median()),
                'std': float(depth_series.std()),
                'min': float(depth_series.min()),
                'max': float(depth_series.max()),
                'q25': float(depth_series.quantile(0.25)),
                'q75': float(depth_series.quantile(0.75))
            }
            
        # ç›´å¾„ç»Ÿè®¡
        if 'diameter' in df.columns and not df['diameter'].empty:
            diameter_series = df['diameter']
            stats['diameter_stats'] = {
                'mean': float(diameter_series.mean()),
                'median': float(diameter_series.median()),
                'std': float(diameter_series.std()),
                'min': float(diameter_series.min()),
                'max': float(diameter_series.max()),
                'q25': float(diameter_series.quantile(0.25)),
                'q75': float(diameter_series.quantile(0.75))
            }
            
        # æ—¶é—´ç»Ÿè®¡
        if 'timestamp' in df.columns and not df['timestamp'].empty:
            time_series = df['timestamp']
            stats['time_stats'] = {
                'start_time': time_series.min().isoformat(),
                'end_time': time_series.max().isoformat(),
                'duration_days': (time_series.max() - time_series.min()).days
            }
            
        return stats
        
    def _analyze_quality_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè´¨é‡æŒ‡æ ‡"""
        quality_stats = {
            'qualified_count': 0,
            'defective_count': 0,
            'qualified_rate': 0.0,
            'defect_rate': 0.0,
            'quality_distribution': {}
        }
        
        if 'status' in df.columns:
            status_counts = df['status'].value_counts()
            
            qualified_count = status_counts.get('qualified', 0)
            defective_count = status_counts.get('defective', 0)
            total_count = len(df)
            
            quality_stats.update({
                'qualified_count': int(qualified_count),
                'defective_count': int(defective_count),
                'qualified_rate': float(qualified_count / total_count * 100) if total_count > 0 else 0.0,
                'defect_rate': float(defective_count / total_count * 100) if total_count > 0 else 0.0,
                'quality_distribution': status_counts.to_dict()
            })
            
        return quality_stats
        
    def _analyze_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        trend_stats = {
            'depth_trend': None,
            'diameter_trend': None,
            'quality_trend': None,
            'time_series': []
        }
        
        if 'timestamp' in df.columns and len(df) > 1:
            # æŒ‰æ—¶é—´æ’åº
            df_sorted = df.sort_values('timestamp')
            
            # æ·±åº¦è¶‹åŠ¿
            if 'depth' in df_sorted.columns:
                depth_trend = self._calculate_trend(df_sorted['depth'].values)
                trend_stats['depth_trend'] = depth_trend
                
            # ç›´å¾„è¶‹åŠ¿
            if 'diameter' in df_sorted.columns:
                diameter_trend = self._calculate_trend(df_sorted['diameter'].values)
                trend_stats['diameter_trend'] = diameter_trend
                
            # æ—¶é—´åºåˆ—æ•°æ®
            trend_stats['time_series'] = self._generate_time_series(df_sorted)
            
        return trend_stats
        
    def _calculate_trend(self, values: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return {'slope': 0.0, 'correlation': 0.0}
            
        x = np.arange(len(values))
        slope, intercept = np.polyfit(x, values, 1)
        correlation = np.corrcoef(x, values)[0, 1]
        
        return {
            'slope': float(slope),
            'intercept': float(intercept),
            'correlation': float(correlation)
        }
        
    def _generate_time_series(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®"""
        time_series = []
        
        # æŒ‰å¤©åˆ†ç»„
        if 'timestamp' in df.columns:
            df['date'] = df['timestamp'].dt.date
            daily_stats = df.groupby('date').agg({
                'depth': ['mean', 'count'],
                'diameter': ['mean', 'count']
            }).reset_index()
            
            for _, row in daily_stats.iterrows():
                time_series.append({
                    'date': row['date'].isoformat(),
                    'avg_depth': float(row[('depth', 'mean')]),
                    'avg_diameter': float(row[('diameter', 'mean')]),
                    'count': int(row[('depth', 'count')])
                })
                
        return time_series
        
    def _detect_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomaly_stats = {
            'anomaly_count': 0,
            'anomaly_rate': 0.0,
            'anomalies': []
        }
        
        # åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹
        for column in ['depth', 'diameter']:
            if column in df.columns:
                series = df[column]
                mean = series.mean()
                std = series.std()
                
                # å¼‚å¸¸å®šä¹‰ï¼šè¶…è¿‡3ä¸ªæ ‡å‡†å·®
                anomalies = df[abs(series - mean) > 3 * std]
                
                for _, anomaly in anomalies.iterrows():
                    anomaly_stats['anomalies'].append({
                        'type': f'{column}_anomaly',
                        'value': float(anomaly[column]),
                        'expected_range': [float(mean - 3*std), float(mean + 3*std)],
                        'timestamp': anomaly.get('timestamp', '').isoformat() if 'timestamp' in anomaly else None
                    })
                    
        anomaly_stats['anomaly_count'] = len(anomaly_stats['anomalies'])
        anomaly_stats['anomaly_rate'] = float(anomaly_stats['anomaly_count'] / len(df) * 100) if len(df) > 0 else 0.0
        
        return anomaly_stats
        
    def _compile_results(self, basic_stats, quality_stats, trend_stats, anomaly_stats) -> Dict[str, Any]:
        """æ•´åˆåˆ†æç»“æœ"""
        return {
            'analysis_timestamp': datetime.now().isoformat(),
            'basic_statistics': basic_stats,
            'quality_metrics': quality_stats,
            'trend_analysis': trend_stats,
            'anomaly_detection': anomaly_stats,
            
            # ä¸ºUIå‡†å¤‡çš„æ±‡æ€»æ•°æ®
            'key_metrics': {
                'total_holes': basic_stats.get('total_count', 0),
                'qualified_rate': quality_stats.get('qualified_rate', 0.0),
                'defect_rate': quality_stats.get('defect_rate', 0.0),
                'avg_depth': basic_stats.get('depth_stats', {}).get('mean', 0.0),
                'avg_diameter': basic_stats.get('diameter_stats', {}).get('mean', 0.0)
            },
            
            'trend_data': trend_stats.get('time_series', []),
            'quality_data': quality_stats.get('quality_distribution', {}),
            'table_data': self._prepare_table_data(basic_stats, quality_stats)
        }
        
    def _prepare_table_data(self, basic_stats, quality_stats) -> List[Dict[str, Any]]:
        """å‡†å¤‡è¡¨æ ¼æ•°æ®"""
        table_data = []
        
        # åŸºç¡€ç»Ÿè®¡è¡Œ
        depth_stats = basic_stats.get('depth_stats', {})
        diameter_stats = basic_stats.get('diameter_stats', {})
        
        table_data.extend([
            {'æŒ‡æ ‡': 'æ·±åº¦å‡å€¼', 'æ•°å€¼': f"{depth_stats.get('mean', 0):.2f} mm"},
            {'æŒ‡æ ‡': 'æ·±åº¦æ ‡å‡†å·®', 'æ•°å€¼': f"{depth_stats.get('std', 0):.2f} mm"},
            {'æŒ‡æ ‡': 'ç›´å¾„å‡å€¼', 'æ•°å€¼': f"{diameter_stats.get('mean', 0):.2f} mm"},
            {'æŒ‡æ ‡': 'ç›´å¾„æ ‡å‡†å·®', 'æ•°å€¼': f"{diameter_stats.get('std', 0):.2f} mm"},
            {'æŒ‡æ ‡': 'åˆæ ¼ç‡', 'æ•°å€¼': f"{quality_stats.get('qualified_rate', 0):.1f}%"},
            {'æŒ‡æ ‡': 'ç¼ºé™·ç‡', 'æ•°å€¼': f"{quality_stats.get('defect_rate', 0):.1f}%"}
        ])
        
        return table_data