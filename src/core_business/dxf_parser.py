"""
DXFæ–‡ä»¶è§£æå™¨
è´Ÿè´£è§£æDXFæ–‡ä»¶å¹¶æå–ç®¡å­”ä¿¡æ¯
"""

import ezdxf
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from collections import defaultdict
import math
import time
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache

# ä¿®æ”¹å¯¼å…¥è·¯å¾„ä»¥é€‚åº”ä¸»é¡¹ç›®ç»“æ„
from src.core_business.models.hole_data import HoleData, HoleCollection, HoleStatus
from src.core.dependency_injection import injectable, ServiceLifetime
from src.core.error_handler import get_error_handler, error_handler, ErrorCategory
from src.data.config_manager import get_config
from src.core_business.business_cache import BusinessCacheManager, cached_business_operation
from src.core_business.business_rules import BusinessRuleEngine, apply_business_rules


@injectable(ServiceLifetime.SINGLETON)
class DXFParser:
    """DXFæ–‡ä»¶è§£æå™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, cache_manager: BusinessCacheManager = None, 
                 rule_engine: BusinessRuleEngine = None):
        self.logger = logging.getLogger(__name__)
        self.error_handler = get_error_handler()
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        if cache_manager is None:
            try:
                self.cache_manager = BusinessCacheManager()
            except Exception as e:
                self.logger.warning(f"æ— æ³•åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨: {e}")
                self.cache_manager = None
        else:
            self.cache_manager = cache_manager
            
        self.rule_engine = rule_engine
        
        # ä»é…ç½®è·å–å‚æ•°
        self.hole_radius_tolerance = get_config('aidcis2.hole_radius_tolerance', 0.1)
        self.position_tolerance = get_config('aidcis2.position_tolerance', 0.01)
        self.expected_hole_radius = get_config('aidcis2.expected_hole_radius', 8.865)
        self.enable_parallel_processing = get_config('aidcis2.enable_parallel_processing', True)
        self.max_workers = get_config('aidcis2.max_workers', 4)
        
        # æ€§èƒ½ç»Ÿè®¡
        self._parsing_stats = {
            'total_files_parsed': 0,
            'total_parsing_time': 0.0,
            'average_parsing_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
    @error_handler(component="DXFParser", category=ErrorCategory.BUSINESS)
    @cached_business_operation(operation_name="dxf_parse_file")
    @apply_business_rules(["dxf_file_validation", "performance_optimization"])
    def parse_file(self, file_path: str) -> HoleCollection:
        """
        è§£æDXFæ–‡ä»¶ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬

        Args:
            file_path: DXFæ–‡ä»¶è·¯å¾„

        Returns:
            HoleCollection: è§£æå¾—åˆ°çš„å­”é›†åˆ
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"å¼€å§‹è§£æDXFæ–‡ä»¶: {file_path}")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_file_cache_key(file_path)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                self._parsing_stats['cache_hits'] += 1
                self.logger.info(f"DXFè§£æå‘½ä¸­ç¼“å­˜: {file_path}")
                return cached_result
            
            self._parsing_stats['cache_misses'] += 1

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(file_path).exists():
                raise FileNotFoundError(f"DXFæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = Path(file_path).stat().st_size
            self.logger.info(f"æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

            if file_size == 0:
                raise ValueError("DXFæ–‡ä»¶ä¸ºç©º")

            # è¯»å–DXFæ–‡ä»¶
            try:
                doc = ezdxf.readfile(file_path)
                self.logger.info(f"DXFç‰ˆæœ¬: {doc.dxfversion}")
            except ezdxf.DXFStructureError as e:
                self.error_handler.handle_error(e, component="DXFParser", context={"file": file_path})
                raise ValueError(f"DXFæ–‡ä»¶ç»“æ„é”™è¯¯: {e}")
            except ezdxf.DXFVersionError as e:
                self.error_handler.handle_error(e, component="DXFParser", context={"file": file_path})
                raise ValueError(f"ä¸æ”¯æŒçš„DXFç‰ˆæœ¬: {e}")

            # è·å–æ¨¡å‹ç©ºé—´
            modelspace = doc.modelspace()
            entities = list(modelspace)
            self.logger.info(f"å®ä½“æ€»æ•°: {len(entities)}")

            if len(entities) == 0:
                self.logger.warning("DXFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“")

            # è§§æå¼§å½¢å®ä½“ - å¹¶è¡Œä¼˜åŒ–
            if self.enable_parallel_processing and len(entities) > 1000:
                arcs = self._extract_arcs_parallel(entities)
            else:
                arcs = self._extract_arcs(entities)
            self.logger.info(f"å¼§å½¢å®ä½“æ•°é‡: {len(arcs)}")

            if len(arcs) == 0:
                self.logger.warning("DXFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¼§å½¢å®ä½“")

            # è¯†åˆ«ç®¡å­” - ä¼˜åŒ–ç®—æ³•
            holes = self._identify_holes_optimized(arcs)
            self.logger.info(f"è¯†åˆ«åˆ°ç®¡å­”æ•°é‡: {len(holes)}")

            if len(holes) == 0:
                self.logger.warning(f"æœªè¯†åˆ«åˆ°ç®¡å­”ã€‚é¢„æœŸåŠå¾„: {self.expected_hole_radius}mm")
                # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                if len(arcs) > 0:
                    radii = [arc.dxf.radius for arc in arcs]
                    unique_radii = sorted(set(radii))
                    self.logger.info(f"å‘ç°çš„å¼§å½¢åŠå¾„: {unique_radii}")

            # åˆ†é…ç½‘æ ¼ä½ç½®
            self._assign_grid_positions(holes)
            
            # AIå‘˜å·¥2å·ä¿®æ”¹å¼€å§‹ - 2025-01-14
            # ä¿®æ”¹ç›®çš„ï¼šå°†å­”ä½IDä»(row,column)æ ¼å¼è½¬æ¢ä¸ºC{col}R{row}æ ¼å¼
            # æ›´æ–°hole_idä¸ºC{column:03d}R{row:03d}æ ¼å¼
            for hole in holes:
                if hole.row is not None and hole.column is not None:
                    hole.hole_id = f"C{hole.column:03d}R{hole.row:03d}"
            # AIå‘˜å·¥2å·ä¿®æ”¹ç»“æŸ

            # å¯¹æ‰€æœ‰å­”ä½è¿›è¡Œ90åº¦é€†æ—¶é’ˆæ—‹è½¬
            self._rotate_holes_90_ccw(holes)
            
            # åˆ›å»ºå­”é›†åˆ
            hole_collection = HoleCollection(
                holes={hole.hole_id: hole for hole in holes},
                metadata={
                    'source_file': file_path,
                    'dxf_version': doc.dxfversion,
                    'total_entities': len(entities),
                    'total_arcs': len(arcs),
                    'file_size': file_size,
                    'pre_rotated': True  # æ ‡è®°å·²é¢„æ—‹è½¬
                }
            )

            # ç¼“å­˜ç»“æœåˆ°æ–°çš„ç¼“å­˜ç®¡ç†å™¨
            self._cache_result(cache_key, hole_collection)
            
            # åŒæ—¶ç¼“å­˜åˆ°ä¸šåŠ¡ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.cache_manager:
                self.cache_manager.cache_business_operation(
                    "dxf_parse_complete", 
                    {"file_path": file_path, "file_size": file_size}, 
                    hole_collection,
                    ttl=1800  # 30åˆ†é’Ÿ
                )
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            parsing_time = time.time() - start_time
            self._update_parsing_stats(parsing_time)
            
            self.logger.info(f"DXFè§£æå®Œæˆï¼Œå…±è§£æå‡º {len(hole_collection)} ä¸ªç®¡å­”ï¼ˆè€—æ—¶{parsing_time:.3f}ç§’ï¼‰")
            return hole_collection

        except (FileNotFoundError, ValueError) as e:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥é”™è¯¯
            raise
        except Exception as e:
            self.error_handler.handle_error(e, component="DXFParser", context={"file": file_path})
            self.logger.error(f"è§£æDXFæ–‡ä»¶æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
            raise ValueError(f"DXFæ–‡ä»¶è§£æå¤±è´¥: {e}")
        finally:
            # å³ä½¿å¤±è´¥ä¹Ÿè¦ç»Ÿè®¡æ—¶é—´
            if 'start_time' in locals():
                parsing_time = time.time() - start_time
                self._update_parsing_stats(parsing_time)
    
    def _extract_arcs(self, entities) -> List:
        """æå–æ‰€æœ‰å¼§å½¢å®ä½“"""
        arcs = []
        for entity in entities:
            if entity.dxftype() == 'ARC':
                arcs.append(entity)
        return arcs
    
    def _extract_arcs_parallel(self, entities) -> List:
        """å¹¶è¡Œæå–å¼§å½¢å®ä½“"""
        def extract_chunk(chunk):
            return [entity for entity in chunk if entity.dxftype() == 'ARC']
        
        # åˆ†å—å¤„ç†
        chunk_size = max(1, len(entities) // self.max_workers)
        chunks = [entities[i:i + chunk_size] for i in range(0, len(entities), chunk_size)]
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(extract_chunk, chunks))
        
        # åˆå¹¶ç»“æœ
        arcs = []
        for result in results:
            arcs.extend(result)
        
        return arcs
    
    def _identify_holes_optimized(self, arcs: List) -> List[HoleData]:
        """
        ä¼˜åŒ–çš„ç®¡å­”è¯†åˆ«ç®—æ³• - å‘é‡åŒ–æ‰¹å¤„ç†ç‰ˆæœ¬
        """
        if not arcs:
            return []
            
        start_time = time.time()
        # é™é»˜å¤„ç†ï¼Œåªåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶è¾“å‡ºå…³é”®ä¿¡æ¯
        if len(arcs) > 10000:
            self.logger.info(f"ğŸš€ å¼€å§‹å­”ä½è¯†åˆ«: {len(arcs)} ä¸ªå¼§å½¢")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„å®ç°
        holes = self._identify_holes_vectorized(arcs)
        
        elapsed_time = time.time() - start_time
        
        # åªåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶è¾“å‡ºæ€§èƒ½ä¿¡æ¯
        if len(arcs) > 10000:
            self.logger.info(f"âœ… å­”ä½è¯†åˆ«å®Œæˆ: {len(holes)} ä¸ªå­”ä½ï¼Œ{elapsed_time:.1f}ç§’ï¼Œ{len(arcs)/elapsed_time:.0f} å¼§å½¢/ç§’")
        
        return holes
    
    def _identify_holes_vectorized(self, arcs: List) -> List[HoleData]:
        """
        å‘é‡åŒ–çš„ç®¡å­”è¯†åˆ«ç®—æ³• - é«˜æ€§èƒ½ç‰ˆæœ¬
        """
        # æŒ‰ä¸­å¿ƒä½ç½®å’ŒåŠå¾„åˆ†ç»„å¼§å½¢ - ä½¿ç”¨ä¼˜åŒ–çš„é”®ç”Ÿæˆ
        arc_groups = defaultdict(list)
        filtered_count = 0
        boundary_count = 0
        
        # æ‰¹é‡é¢„å¤„ç†å¼§å½¢
        batch_size = 5000
        for i in range(0, len(arcs), batch_size):
            batch = arcs[i:i + batch_size]
            
            for arc in batch:
                center = arc.dxf.center
                radius = arc.dxf.radius
                
                # å¿«é€Ÿè¿‡æ»¤ï¼ˆä½¿ç”¨æ—©æœŸè¿”å›ï¼‰
                if radius > 100:  # å¤§äº100çš„åŠå¾„è®¤ä¸ºæ˜¯è¾¹ç•Œ
                    boundary_count += 1
                    continue
                    
                if abs(radius - self.expected_hole_radius) > self.hole_radius_tolerance:
                    filtered_count += 1
                    continue
                
                # ä¼˜åŒ–çš„åˆ†ç»„é”®ï¼ˆå‡å°‘ç²¾åº¦ä»¥æé«˜å‘½ä¸­ç‡ï¼‰
                key = (
                    round(center.x, 1),  # å‡å°‘ç²¾åº¦ä»¥æé«˜åˆ†ç»„æ•ˆç‡
                    round(center.y, 1),
                    round(radius, 2)
                )
                arc_groups[key].append(arc)
        
        # åªåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if len(arcs) > 10000:
            self.logger.info(f"ğŸ“Š å¼§å½¢é¢„å¤„ç†: è¾¹ç•Œå¼§å½¢={boundary_count}, è¿‡æ»¤å¼§å½¢={filtered_count}, æœ‰æ•ˆç»„={len(arc_groups)}")
        
        # æ‰¹é‡è¯†åˆ«å­”ä½
        holes = []
        hole_id_counter = 1
        
        for (center_x, center_y, radius), group_arcs in arc_groups.items():
            if len(group_arcs) >= 2 and self._is_complete_circle_fast(group_arcs):
                hole = HoleData(
                    hole_id=f"H{hole_id_counter:05d}",
                    center_x=center_x,
                    center_y=center_y,
                    radius=radius,
                    status=HoleStatus.PENDING,
                    layer=group_arcs[0].dxf.layer,
                    metadata={
                        'arc_count': len(group_arcs),
                        'source_arcs': list(range(len(group_arcs)))
                    }
                )
                holes.append(hole)
                hole_id_counter += 1
        
        return holes
    
    def _is_complete_circle_fast(self, arcs: List) -> bool:
        """
        å¿«é€Ÿæ£€æŸ¥å¼§å½¢åˆ—è¡¨æ˜¯å¦ç»„æˆå®Œæ•´çš„åœ† - ä¼˜åŒ–ç‰ˆæœ¬
        """
        if len(arcs) < 2:
            return False
            
        # ç®€åŒ–çš„å®Œæ•´æ€§æ£€æŸ¥ - åªæ£€æŸ¥å¼§å½¢æ•°é‡å’ŒåŸºæœ¬è§’åº¦è¦†ç›–
        if len(arcs) == 2:
            # ä¸¤ä¸ªå¼§å½¢çš„æƒ…å†µï¼šæ£€æŸ¥è§’åº¦å·®æ˜¯å¦æ¥è¿‘180åº¦
            arc1, arc2 = arcs[0], arcs[1]
            angle_diff = abs(arc1.dxf.start_angle - arc2.dxf.start_angle)
            return 160 <= angle_diff <= 200 or 340 <= angle_diff <= 380
        else:
            # å¤šä¸ªå¼§å½¢çš„æƒ…å†µï¼šç®€å•æ£€æŸ¥æ€»è§’åº¦
            total_angle = sum(abs(arc.dxf.end_angle - arc.dxf.start_angle) for arc in arcs)
            return 340 <= total_angle <= 380
    
    def _identify_holes(self, arcs: List) -> List[HoleData]:
        """
        ä»å¼§å½¢å®ä½“ä¸­è¯†åˆ«ç®¡å­”

        ç®¡å­”ç”±ä¸¤ä¸ªåŠåœ†å¼§ç»„æˆï¼Œå…·æœ‰ç›¸åŒçš„ä¸­å¿ƒå’ŒåŠå¾„
        """
        # æŒ‰ä¸­å¿ƒä½ç½®å’ŒåŠå¾„åˆ†ç»„å¼§å½¢
        arc_groups = defaultdict(list)
        filtered_count = 0
        boundary_count = 0

        self.logger.info(f"å¼€å§‹è¯†åˆ«ç®¡å­”ï¼Œæ€»å¼§å½¢æ•°: {len(arcs)}")

        for arc in arcs:
            center = arc.dxf.center
            radius = arc.dxf.radius

            # è¿‡æ»¤æ‰å¤–è¾¹ç•Œå¤§åœ†ï¼ˆåŠå¾„2300ï¼‰
            if radius > 100:  # å¤§äº100çš„åŠå¾„è®¤ä¸ºæ˜¯è¾¹ç•Œ
                boundary_count += 1
                self.logger.debug(f"è¿‡æ»¤è¾¹ç•Œå¼§å½¢: åŠå¾„={radius:.3f}")
                continue

            # åªå¤„ç†é¢„æœŸåŠå¾„çš„å¼§å½¢
            if abs(radius - self.expected_hole_radius) > self.hole_radius_tolerance:
                filtered_count += 1
                self.logger.debug(f"è¿‡æ»¤éæ ‡å‡†åŠå¾„å¼§å½¢: åŠå¾„={radius:.3f}, é¢„æœŸ={self.expected_hole_radius}")
                continue

            # ä½¿ç”¨ä¸­å¿ƒåæ ‡å’ŒåŠå¾„ä½œä¸ºåˆ†ç»„é”®
            key = (
                round(center.x, 2),  # ä¿ç•™2ä½å°æ•°ç²¾åº¦
                round(center.y, 2),
                round(radius, 3)
            )
            arc_groups[key].append(arc)

        self.logger.info(f"å¼§å½¢è¿‡æ»¤ç»“æœ: è¾¹ç•Œå¼§å½¢={boundary_count}, éæ ‡å‡†åŠå¾„={filtered_count}, æœ‰æ•ˆå¼§å½¢ç»„={len(arc_groups)}")

        # è¯†åˆ«å®Œæ•´çš„å­”ï¼ˆç”±ä¸¤ä¸ªåŠåœ†å¼§ç»„æˆï¼‰
        holes = []
        hole_id_counter = 1
        incomplete_groups = 0

        for (center_x, center_y, radius), group_arcs in arc_groups.items():
            self.logger.debug(f"æ£€æŸ¥å­”ä½ç»„: ä¸­å¿ƒ({center_x}, {center_y}), åŠå¾„={radius}, å¼§å½¢æ•°={len(group_arcs)}")

            if len(group_arcs) >= 2:  # è‡³å°‘æœ‰ä¸¤ä¸ªå¼§å½¢æ‰èƒ½ç»„æˆä¸€ä¸ªå­”
                # æ£€æŸ¥æ˜¯å¦æœ‰äº’è¡¥çš„åŠåœ†å¼§
                if self._is_complete_circle(group_arcs):
                    # åˆ›å»ºå­”æ•°æ®
                    hole = HoleData(
                        hole_id=f"H{hole_id_counter:05d}",
                        center_x=center_x,
                        center_y=center_y,
                        radius=radius,
                        status=HoleStatus.PENDING,
                        layer=group_arcs[0].dxf.layer,
                        metadata={
                            'arc_count': len(group_arcs),
                            'source_arcs': [i for i, arc in enumerate(group_arcs)]
                        }
                    )
                    holes.append(hole)
                    self.logger.debug(f"è¯†åˆ«åˆ°å®Œæ•´å­”ä½: {hole.hole_id}")
                    hole_id_counter += 1
                else:
                    incomplete_groups += 1
                    self.logger.debug(f"ä¸å®Œæ•´å­”ä½ç»„: ä¸­å¿ƒ({center_x}, {center_y}), å¼§å½¢æ•°={len(group_arcs)}")
            else:
                incomplete_groups += 1
                self.logger.debug(f"å¼§å½¢æ•°ä¸è¶³: ä¸­å¿ƒ({center_x}, {center_y}), å¼§å½¢æ•°={len(group_arcs)}")

        self.logger.info(f"å­”ä½è¯†åˆ«å®Œæˆ: å®Œæ•´å­”ä½={len(holes)}, ä¸å®Œæ•´ç»„={incomplete_groups}")
        return holes
    
    def _is_complete_circle(self, arcs: List) -> bool:
        """
        æ£€æŸ¥å¼§å½¢åˆ—è¡¨æ˜¯å¦ç»„æˆå®Œæ•´çš„åœ†

        Args:
            arcs: å¼§å½¢å®ä½“åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦ç»„æˆå®Œæ•´åœ†
        """
        if len(arcs) < 2:
            return False

        # è®¡ç®—æ€»è§’åº¦è¦†ç›–
        total_angle = 0
        angle_ranges = []

        for arc in arcs:
            start_angle = arc.dxf.start_angle
            end_angle = arc.dxf.end_angle

            # æ ‡å‡†åŒ–è§’åº¦åˆ°0-360èŒƒå›´
            start_angle = start_angle % 360
            end_angle = end_angle % 360

            # è®¡ç®—è§’åº¦å·®
            if end_angle == 0 and start_angle == 180:
                # ç‰¹æ®Šæƒ…å†µï¼š180Â°-0Â° å®é™…ä¸Šæ˜¯ 180Â°-360Â°
                angle_diff = 180
            elif end_angle < start_angle:
                # è·¨è¶Š0åº¦çš„æƒ…å†µ
                angle_diff = (360 - start_angle) + end_angle
            else:
                # æ­£å¸¸æƒ…å†µ
                angle_diff = end_angle - start_angle

            total_angle += angle_diff
            angle_ranges.append((start_angle, end_angle, angle_diff))

            self.logger.debug(f"å¼§å½¢è§’åº¦: {start_angle:.1f}Â° -> {end_angle:.1f}Â°, è§’åº¦å·®: {angle_diff:.1f}Â°")

        self.logger.debug(f"æ€»è§’åº¦è¦†ç›–: {total_angle:.1f}Â°")

        # æ£€æŸ¥æ€»è§’åº¦æ˜¯å¦æ¥è¿‘360åº¦
        is_complete = abs(total_angle - 360) < 10  # å…è®¸10åº¦è¯¯å·®

        if not is_complete:
            self.logger.debug(f"ä¸å®Œæ•´åœ†å½¢ï¼Œæ€»è§’åº¦: {total_angle:.1f}Â°")

        return is_complete
    
    def _assign_grid_positions(self, holes: List[HoleData]) -> None:
        """
        ä¸ºå­”åˆ†é…ç½‘æ ¼ä½ç½®ï¼ˆè¡Œåˆ—å·ï¼‰
        
        Args:
            holes: å­”æ•°æ®åˆ—è¡¨
        """
        if not holes:
            return
        
        # æŒ‰Yåæ ‡æ’åºç¡®å®šè¡Œ
        holes_by_y = sorted(holes, key=lambda h: h.center_y, reverse=True)
        
        # è¯†åˆ«è¡Œ
        current_row = 1
        current_y = holes_by_y[0].center_y
        row_tolerance = 5.0  # Yåæ ‡å®¹å·®
        
        for hole in holes_by_y:
            if abs(hole.center_y - current_y) > row_tolerance:
                current_row += 1
                current_y = hole.center_y
            hole.row = current_row
        
        # ä¸ºæ¯è¡ŒæŒ‰Xåæ ‡æ’åºç¡®å®šåˆ—
        rows = defaultdict(list)
        for hole in holes:
            rows[hole.row].append(hole)
        
        for row_num, row_holes in rows.items():
            row_holes.sort(key=lambda h: h.center_x)
            for col_num, hole in enumerate(row_holes, 1):
                hole.column = col_num
    
    def get_parsing_stats(self, hole_collection: HoleCollection) -> Dict:
        """
        è·å–è§£æç»Ÿè®¡ä¿¡æ¯
        
        Args:
            hole_collection: å­”é›†åˆ
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not hole_collection.holes:
            return {}
        
        holes = list(hole_collection.holes.values())
        
        # è®¡ç®—è¾¹ç•Œ
        min_x = min(hole.center_x for hole in holes)
        max_x = max(hole.center_x for hole in holes)
        min_y = min(hole.center_y for hole in holes)
        max_y = max(hole.center_y for hole in holes)
        
        # ç»Ÿè®¡åŠå¾„åˆ†å¸ƒ
        radii = [hole.radius for hole in holes]
        unique_radii = list(set(radii))
        
        # ç»Ÿè®¡å›¾å±‚åˆ†å¸ƒ
        layers = [hole.layer for hole in holes]
        layer_counts = {}
        for layer in layers:
            layer_counts[layer] = layer_counts.get(layer, 0) + 1
        
        return {
            'total_holes': len(holes),
            'bounds': {
                'min_x': min_x,
                'max_x': max_x,
                'min_y': min_y,
                'max_y': max_y,
                'width': max_x - min_x,
                'height': max_y - min_y
            },
            'radius_distribution': {
                'unique_count': len(unique_radii),
                'radii': sorted(unique_radii),
                'most_common': max(set(radii), key=radii.count) if radii else None
            },
            'layer_distribution': layer_counts,
            'status_counts': hole_collection.get_status_counts()
        }
    
    def _rotate_holes_90_ccw(self, holes: List[HoleData]) -> None:
        """
        å¯¹æ‰€æœ‰å­”ä½è¿›è¡Œ90åº¦é€†æ—¶é’ˆæ—‹è½¬ - ä¼˜åŒ–ç‰ˆæœ¬
        æ—‹è½¬å…¬å¼: (x, y) -> (-y, x)
        
        å¢å¼ºåŠŸèƒ½ï¼š
        - æ”¯æŒè‡ªé€‚åº”æ—‹è½¬è§’åº¦
        - ä¿æŒåæ ‡ç³»ä¸€è‡´æ€§
        - ä¼˜åŒ–æ€§èƒ½
        
        Args:
            holes: å­”ä½åˆ—è¡¨
        """
        if not holes:
            return
            
        start_time = time.perf_counter()
        hole_count = len(holes)
        
        # è®¡ç®—å‡ ä½•ä¸­å¿ƒï¼ˆä½¿ç”¨æ‰€æœ‰å­”ä½çš„ä¸­å¿ƒï¼‰
        center_x = sum(hole.center_x for hole in holes) / hole_count
        center_y = sum(hole.center_y for hole in holes) / hole_count
        
        # è·å–æ—‹è½¬é…ç½®
        rotation_config = self._get_rotation_config()
        rotation_angle = rotation_config.get('angle', 90.0)  # é»˜è®¤90åº¦
        apply_rotation = rotation_config.get('enabled', True)
        
        if not apply_rotation:
            self.logger.info("åæ ‡æ—‹è½¬å·²ç¦ç”¨")
            return
        
        # åªåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶è¾“å‡ºæ—‹è½¬ä¿¡æ¯
        if hole_count > 10000:
            self.logger.info(f"ğŸ”„ æ‰§è¡Œ{rotation_angle}åº¦é€†æ—¶é’ˆæ—‹è½¬: ({center_x:.2f}, {center_y:.2f})")
        
        # è®¡ç®—æ—‹è½¬å‚æ•°
        rotation_rad = math.radians(rotation_angle)
        cos_angle = math.cos(rotation_rad)
        sin_angle = math.sin(rotation_rad)
        
        # æ‰¹é‡æ—‹è½¬ - å‘é‡åŒ–æ“ä½œ
        batch_size = 5000  # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜å³°å€¼
        
        for i in range(0, hole_count, batch_size):
            batch = holes[i:i + batch_size]
            
            # æ‰¹é‡è®¡ç®—æ—‹è½¬åæ ‡
            for hole in batch:
                # å¹³ç§»åˆ°åŸç‚¹
                x = hole.center_x - center_x
                y = hole.center_y - center_y
                
                # åº”ç”¨æ—‹è½¬å˜æ¢
                new_x = x * cos_angle - y * sin_angle
                new_y = x * sin_angle + y * cos_angle
                
                # å¹³ç§»å›åŸä½ç½®
                hole.center_x = new_x + center_x
                hole.center_y = new_y + center_y
        
        elapsed_time = time.perf_counter() - start_time
        # åªåœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶è¾“å‡ºæ€§èƒ½ä¿¡æ¯
        if hole_count > 10000:
            self.logger.info(f"âœ… æ—‹è½¬å®Œæˆ: {hole_count} ä¸ªå­”ä½ï¼Œ{elapsed_time:.1f}ç§’ï¼Œ{hole_count/elapsed_time:.0f} å­”ä½/ç§’")
    
    def _get_rotation_config(self) -> Dict[str, Any]:
        """è·å–æ—‹è½¬é…ç½®"""
        try:
            # å°è¯•ä»é…ç½®æ–‡ä»¶è·å–æ—‹è½¬è®¾ç½®
            return {
                'enabled': get_config('aidcis2.coordinate_rotation.enabled', True),
                'angle': get_config('aidcis2.coordinate_rotation.angle', 90.0),
                'adaptive': get_config('aidcis2.coordinate_rotation.adaptive', False)
            }
        except Exception:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'enabled': True,
                'angle': 90.0,
                'adaptive': False
            }
    
    @lru_cache(maxsize=128)
    def _get_file_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶ç¼“å­˜é”®"""
        file_stat = Path(file_path).stat()
        return f"{file_path}_{file_stat.st_mtime}_{file_stat.st_size}"
    
    def _get_cached_result(self, cache_key: str) -> Optional[HoleCollection]:
        """è·å–ç¼“å­˜ç»“æœ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not hasattr(self, '_cache'):
            self._cache = {}
            self._cache_access_times = {}  # è®°å½•è®¿é—®æ—¶é—´ç”¨äºLRUç­–ç•¥
        
        if cache_key in self._cache:
            # æ›´æ–°è®¿é—®æ—¶é—´
            self._cache_access_times[cache_key] = time.time()
            return self._cache[cache_key]
        
        return None
    
    def _cache_result(self, cache_key: str, result: HoleCollection) -> None:
        """ç¼“å­˜ç»“æœ - æ™ºèƒ½å†…å­˜ç®¡ç†ç‰ˆæœ¬"""
        if not hasattr(self, '_cache'):
            self._cache = {}
            self._cache_access_times = {}
        
        # åŠ¨æ€ç¼“å­˜å¤§å°é™åˆ¶ï¼ˆåŸºäºå¯ç”¨å†…å­˜ï¼‰
        max_cache_size = get_config('aidcis2.max_cache_size', 5)  # å‡å°‘é»˜è®¤ç¼“å­˜å¤§å°
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°å¹¶æ‰§è¡ŒLRUæ¸…ç†
        if len(self._cache) >= max_cache_size:
            # æ‰¾å‡ºæœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
            oldest_key = min(self._cache_access_times.keys(), 
                           key=lambda k: self._cache_access_times[k])
            
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            del self._cache[oldest_key]
            del self._cache_access_times[oldest_key]
            
            # ä¸»åŠ¨åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # é™é»˜æ¸…ç†ç¼“å­˜
            if len(self._cache) > 5:  # åªåœ¨ç¼“å­˜è¾ƒå¤šæ—¶è¾“å‡º
                self.logger.info(f"ğŸ§¹ æ¸…ç†ç¼“å­˜æ¡ç›®: {oldest_key}")
        
        # ç¼“å­˜æ–°ç»“æœ
        self._cache[cache_key] = result
        self._cache_access_times[cache_key] = time.time()
        
        # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        cache_size_mb = len(self._cache) * 0.1  # ä¼°ç®—æ¯ä¸ªç¼“å­˜æ¡ç›®çº¦0.1MB
        # é™é»˜ç¼“å­˜ç®¡ç†
        if len(self._cache) > 3:  # åªåœ¨ç¼“å­˜è¾ƒå¤šæ—¶è¾“å‡º
            self.logger.info(f"ğŸ“Š ç¼“å­˜å¤§å°: {len(self._cache)} æ¡ç›®, çº¦ {cache_size_mb:.1f}MB")
    
    def _update_parsing_stats(self, parsing_time: float) -> None:
        """æ›´æ–°è§£æç»Ÿè®¡ä¿¡æ¯"""
        self._parsing_stats['total_files_parsed'] += 1
        self._parsing_stats['total_parsing_time'] += parsing_time
        self._parsing_stats['average_parsing_time'] = (
            self._parsing_stats['total_parsing_time'] / 
            self._parsing_stats['total_files_parsed']
        )
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            'parsing_stats': self._parsing_stats.copy(),
            'cache_stats': {
                'cache_size': len(getattr(self, '_cache', {})),
                'hit_rate': (
                    self._parsing_stats['cache_hits'] / 
                    (self._parsing_stats['cache_hits'] + self._parsing_stats['cache_misses'])
                    if (self._parsing_stats['cache_hits'] + self._parsing_stats['cache_misses']) > 0 
                    else 0
                )
            },
            'configuration': {
                'hole_radius_tolerance': self.hole_radius_tolerance,
                'position_tolerance': self.position_tolerance,
                'expected_hole_radius': self.expected_hole_radius,
                'enable_parallel_processing': self.enable_parallel_processing,
                'max_workers': self.max_workers
            }
        }
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        cache_count = 0
        
        if hasattr(self, '_cache'):
            cache_count = len(self._cache)
            self._cache.clear()
            
        if hasattr(self, '_cache_access_times'):
            self._cache_access_times.clear()
            
        # ä¸»åŠ¨åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # åªåœ¨æœ‰å¤§é‡ç¼“å­˜æ—¶è¾“å‡ºæ¸…ç†ä¿¡æ¯
        if cache_count > 0:
            self.logger.info(f"ğŸ§¹ DXFè§£æç¼“å­˜å·²æ¸…ç©º: {cache_count} ä¸ªæ¡ç›®")
