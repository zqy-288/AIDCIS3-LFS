"""
Pytesté…ç½®æ–‡ä»¶ - AIDCIS3-LFSé›¶å®¹å¿æµ‹è¯•æ¡†æ¶
Global test fixtures and configuration
"""

import pytest
import sys
import os
import time
import logging
from pathlib import Path
from typing import Generator, Optional
from playwright.sync_api import Playwright, Browser, BrowserContext, Page
from PySide6.QtWidgets import QApplication
from PySide6.QtCore import QTimer
from PySide6.QtTest import QTest

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from playwright_config import CONFIG, TestEnvironment, RETRY_CONFIG, PERFORMANCE_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(PROJECT_ROOT / 'tests' / 'output' / 'test.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class TestSession:
    """æµ‹è¯•ä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.app: Optional[QApplication] = None
        self.main_window = None
        self.test_env = TestEnvironment()
        self.start_time = time.time()
        self.test_count = 0
        self.failed_count = 0
        
    def setup_qt_app(self):
        """è®¾ç½®Qtåº”ç”¨ç¨‹åº"""
        if QApplication.instance() is None:
            self.app = QApplication(sys.argv)
            self.app.setApplicationName("AIDCIS3-LFS-Test")
            self.app.setQuitOnLastWindowClosed(False)
        else:
            self.app = QApplication.instance()
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
        os.environ['QT_QPA_PLATFORM'] = 'offscreen'  # æ— å¤´æ¨¡å¼
        
        return self.app
        
    def create_main_window(self):
        """åˆ›å»ºä¸»çª—å£å®ä¾‹"""
        try:
            from src.main_window import MainWindow
            self.main_window = MainWindow()
            return self.main_window
        except Exception as e:
            logger.error(f"Failed to create main window: {e}")
            raise
            
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.main_window:
            self.main_window.close()
            self.main_window = None
            
        if self.app:
            self.app.processEvents()
            QTimer.singleShot(100, self.app.quit)
            self.app = None

# å…¨å±€æµ‹è¯•ä¼šè¯
test_session = TestSession()

@pytest.fixture(scope="session")
def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ - ä¼šè¯çº§åˆ«"""
    logger.info("ğŸš€ Starting AIDCIS3-LFS test session...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dirs = [
        PROJECT_ROOT / 'tests' / 'output',
        PROJECT_ROOT / 'tests' / 'reports',
        PROJECT_ROOT / 'tests' / 'screenshots',
        PROJECT_ROOT / 'tests' / 'videos',
        PROJECT_ROOT / 'tests' / 'traces'
    ]
    
    for dir_path in output_dirs:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®Qtåº”ç”¨ç¨‹åº
    test_session.setup_qt_app()
    
    yield test_session
    
    # æ¸…ç†
    logger.info("ğŸ§¹ Cleaning up test environment...")
    test_session.cleanup()
    
    # æŠ¥å‘Šæµ‹è¯•ä¼šè¯ç»Ÿè®¡
    duration = time.time() - test_session.start_time
    logger.info(f"ğŸ“Š Test session completed in {duration:.2f}s")
    logger.info(f"ğŸ“‹ Total tests: {test_session.test_count}")
    logger.info(f"âŒ Failed tests: {test_session.failed_count}")

@pytest.fixture(scope="function")
def qt_app(setup_test_environment):
    """Qtåº”ç”¨ç¨‹åºfixture"""
    return setup_test_environment.app

@pytest.fixture(scope="function")
def main_window(qt_app):
    """ä¸»çª—å£fixture"""
    window = test_session.create_main_window()
    yield window
    
    # æ¸…ç†çª—å£
    if window:
        window.close()
        qt_app.processEvents()

@pytest.fixture(scope="session")
def playwright() -> Generator[Playwright, None, None]:
    """Playwrightå®ä¾‹fixture"""
    from playwright.sync_api import sync_playwright
    
    with sync_playwright() as p:
        yield p

@pytest.fixture(scope="function")
def browser(playwright: Playwright) -> Generator[Browser, None, None]:
    """æµè§ˆå™¨å®ä¾‹fixture"""
    browser = playwright.chromium.launch(
        headless=CONFIG.headless,
        slow_mo=CONFIG.slow_mo
    )
    yield browser
    browser.close()

@pytest.fixture(scope="function")
def context(browser: Browser) -> Generator[BrowserContext, None, None]:
    """æµè§ˆå™¨ä¸Šä¸‹æ–‡fixture"""
    context = browser.new_context(
        viewport={'width': CONFIG.viewport_width, 'height': CONFIG.viewport_height},
        record_video_dir=str(PROJECT_ROOT / 'tests' / 'output' / 'videos')
    )
    yield context
    context.close()

@pytest.fixture(scope="function")
def page(context: BrowserContext) -> Generator[Page, None, None]:
    """é¡µé¢å®ä¾‹fixture"""
    page = context.new_page()
    page.set_default_timeout(CONFIG.timeout)
    yield page
    page.close()

@pytest.fixture(scope="function")
def test_data_dir():
    """æµ‹è¯•æ•°æ®ç›®å½•fixture"""
    return PROJECT_ROOT / 'tests' / 'test_data'

@pytest.fixture(scope="function")
def dxf_test_files(test_data_dir):
    """DXFæµ‹è¯•æ–‡ä»¶fixture"""
    dxf_files = [
        PROJECT_ROOT / 'test_data.dxf',
        PROJECT_ROOT / 'assets' / 'dxf' / 'DXF Graph' / 'æµ‹è¯•ç®¡æ¿.dxf',
        PROJECT_ROOT / 'assets' / 'dxf' / 'DXF Graph' / 'ä¸œé‡ç®¡æ¿.dxf'
    ]
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    existing_files = [f for f in dxf_files if f.exists()]
    
    if not existing_files:
        pytest.skip("No DXF test files found")
    
    return existing_files

@pytest.fixture(autouse=True)
def test_tracker(request):
    """æµ‹è¯•è·Ÿè¸ªå™¨ - è‡ªåŠ¨æ”¶é›†æµ‹è¯•ç»Ÿè®¡"""
    test_session.test_count += 1
    
    start_time = time.time()
    logger.info(f"ğŸ§ª Starting test: {request.node.name}")
    
    yield
    
    duration = time.time() - start_time
    
    if hasattr(request.node, 'rep_call') and request.node.rep_call.failed:
        test_session.failed_count += 1
        logger.error(f"âŒ Test failed: {request.node.name} (duration: {duration:.3f}s)")
    else:
        logger.info(f"âœ… Test passed: {request.node.name} (duration: {duration:.3f}s)")

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """æ”¶é›†æµ‹è¯•æŠ¥å‘Šä¿¡æ¯"""
    outcome = yield
    rep = outcome.get_result()
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æµ‹è¯•é¡¹ç›®ä¸­ï¼Œç”¨äºåç»­å¤„ç†
    setattr(item, f"rep_{rep.when}", rep)

@pytest.fixture(scope="function")
def performance_monitor():
    """æ€§èƒ½ç›‘æ§fixture"""
    import psutil
    import threading
    
    class PerformanceMonitor:
        def __init__(self):
            self.cpu_usage = []
            self.memory_usage = []
            self.monitoring = False
            self.monitor_thread = None
            
        def start(self):
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitor)
            self.monitor_thread.start()
            
        def stop(self):
            self.monitoring = False
            if self.monitor_thread:
                self.monitor_thread.join()
                
        def _monitor(self):
            while self.monitoring:
                self.cpu_usage.append(psutil.cpu_percent())
                self.memory_usage.append(psutil.virtual_memory().percent)
                time.sleep(0.1)
                
        def get_stats(self):
            return {
                'avg_cpu': sum(self.cpu_usage) / len(self.cpu_usage) if self.cpu_usage else 0,
                'max_cpu': max(self.cpu_usage) if self.cpu_usage else 0,
                'avg_memory': sum(self.memory_usage) / len(self.memory_usage) if self.memory_usage else 0,
                'max_memory': max(self.memory_usage) if self.memory_usage else 0,
            }
    
    monitor = PerformanceMonitor()
    monitor.start()
    
    yield monitor
    
    monitor.stop()
    stats = monitor.get_stats()
    
    # æ£€æŸ¥æ€§èƒ½é˜ˆå€¼
    if stats['max_cpu'] > 80.0:  # 80% CPU threshold
        logger.warning(f"âš ï¸ High CPU usage detected: {stats['max_cpu']:.1f}%")
    
    if stats['max_memory'] > PERFORMANCE_CONFIG.memory_threshold_mb:
        logger.warning(f"âš ï¸ High memory usage detected: {stats['max_memory']:.1f}MB")

@pytest.fixture(scope="function")
def screenshot_on_failure(request, qt_app):
    """å¤±è´¥æ—¶è‡ªåŠ¨æˆªå›¾fixture"""
    yield
    
    if hasattr(request.node, 'rep_call') and request.node.rep_call.failed:
        timestamp = int(time.time())
        screenshot_path = PROJECT_ROOT / 'tests' / 'screenshots' / f'failure_{request.node.name}_{timestamp}.png'
        
        try:
            # å°è¯•æˆªå›¾
            if hasattr(qt_app, 'primaryScreen'):
                screen = qt_app.primaryScreen()
                pixmap = screen.grabWindow(0)
                pixmap.save(str(screenshot_path))
                logger.info(f"ğŸ“¸ Failure screenshot saved: {screenshot_path}")
        except Exception as e:
            logger.warning(f"Failed to capture screenshot: {e}")

# è‡ªå®šä¹‰æ ‡è®°
def pytest_configure(config):
    """é…ç½®è‡ªå®šä¹‰æ ‡è®°"""
    config.addinivalue_line("markers", "ui: UI related tests")
    config.addinivalue_line("markers", "integration: Integration tests")
    config.addinivalue_line("markers", "performance: Performance tests")
    config.addinivalue_line("markers", "slow: Slow running tests")
    config.addinivalue_line("markers", "critical: Critical functionality tests")

# æµ‹è¯•æ”¶é›†é’©å­
def pytest_collection_modifyitems(config, items):
    """ä¿®æ”¹æµ‹è¯•æ”¶é›†"""
    for item in items:
        # ä¸ºæ…¢é€Ÿæµ‹è¯•æ·»åŠ æ ‡è®°
        if "performance" in item.nodeid or "large_dataset" in item.nodeid:
            item.add_marker(pytest.mark.slow)
            
        # ä¸ºå…³é”®æµ‹è¯•æ·»åŠ æ ‡è®°
        if "main_window" in item.nodeid or "dxf_loading" in item.nodeid:
            item.add_marker(pytest.mark.critical)

# å‘½ä»¤è¡Œé€‰é¡¹
def pytest_addoption(parser):
    """æ·»åŠ å‘½ä»¤è¡Œé€‰é¡¹"""
    parser.addoption(
        "--run-slow", action="store_true", default=False,
        help="run slow tests"
    )
    parser.addoption(
        "--performance-only", action="store_true", default=False,
        help="run only performance tests"
    )
    parser.addoption(
        "--critical-only", action="store_true", default=False,
        help="run only critical tests"
    )

def pytest_runtest_setup(item):
    """æµ‹è¯•è¿è¡Œè®¾ç½®"""
    if "slow" in item.keywords and not item.config.getoption("--run-slow"):
        pytest.skip("need --run-slow option to run")
        
    if item.config.getoption("--performance-only") and "performance" not in item.keywords:
        pytest.skip("skipping non-performance test")
        
    if item.config.getoption("--critical-only") and "critical" not in item.keywords:
        pytest.skip("skipping non-critical test")