#!/usr/bin/env python3
"""
å†å²æ•°æ®åç«¯åŠŸèƒ½æµ‹è¯•
ä¸“æ³¨äºæ•°æ®å¤„ç†é€»è¾‘ï¼Œä¸ä¾èµ–GUIç»„ä»¶
"""

import sys
import os
import numpy as np
from datetime import datetime
import csv
import json

# æ·»åŠ æ¨¡å—è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
sys.path.insert(0, os.path.join(current_dir, 'modules'))
sys.path.insert(0, os.path.join(current_dir, 'aidcis2'))

def test_database_operations():
    """æµ‹è¯•æ•°æ®åº“æ“ä½œ"""
    print("ğŸ”§ æµ‹è¯•æ•°æ®åº“æ“ä½œ...")
    
    try:
        from modules.models import db_manager
        
        # åˆå§‹åŒ–æ•°æ®åº“
        print("ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“...")
        db_manager.create_sample_data()
        
        # æ·»åŠ æµ‹è¯•æ•°æ®
        print("ğŸ“ æ·»åŠ æµ‹è¯•æ•°æ®...")
        success_count = 0
        test_data = []
        
        for i in range(15):
            depth = i * 3.0
            diameter = 17.6 + 0.05 * np.sin(depth * 0.2) + np.random.normal(0, 0.01)
            operator = f"æ“ä½œå‘˜{i%3+1}"
            
            success = db_manager.add_measurement_data("H001", depth, diameter, operator)
            if success:
                success_count += 1
                test_data.append({
                    'depth': depth,
                    'diameter': diameter,
                    'operator': operator
                })
                print(f"  âœ… æ·±åº¦={depth:.1f}mm, ç›´å¾„={diameter:.3f}mm, æ“ä½œå‘˜={operator}")
        
        print(f"ğŸ“Š æˆåŠŸæ·»åŠ  {success_count}/15 æ¡æµ‹é‡æ•°æ®")
        
        # æŸ¥è¯¢æ•°æ®
        print("ğŸ” æŸ¥è¯¢å†å²æ•°æ®...")
        measurements = db_manager.get_hole_measurements("H001")
        print(f"  ğŸ“Š H001çš„æµ‹é‡æ•°æ®: {len(measurements)}æ¡")
        
        if measurements:
            print("  ğŸ“‹ æœ€æ–°5æ¡æ•°æ®:")
            for i, m in enumerate(measurements[-5:]):
                print(f"    {i+1}. æ·±åº¦: {m.depth:.1f}mm, ç›´å¾„: {m.diameter:.3f}mm, æ“ä½œå‘˜: {m.operator}")
        
        # ç»Ÿè®¡åˆ†æ
        if measurements:
            diameters = [m.diameter for m in measurements]
            print(f"  ğŸ“ˆ ç›´å¾„ç»Ÿè®¡:")
            print(f"    æœ€å¤§å€¼: {max(diameters):.3f}mm")
            print(f"    æœ€å°å€¼: {min(diameters):.3f}mm")
            print(f"    å¹³å‡å€¼: {np.mean(diameters):.3f}mm")
            print(f"    æ ‡å‡†å·®: {np.std(diameters):.3f}mm")
        
        return True, test_data
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, []

def test_csv_data_processing():
    """æµ‹è¯•CSVæ•°æ®å¤„ç†"""
    print("ğŸ“„ æµ‹è¯•CSVæ•°æ®å¤„ç†...")
    
    try:
        data_summary = {}
        
        # æ£€æŸ¥æ•°æ®ç›®å½•
        data_dirs = ["Data/H00001/CCIDM", "Data/H00002/CCIDM"]
        
        for data_dir in data_dirs:
            hole_id = os.path.basename(os.path.dirname(data_dir))
            data_summary[hole_id] = {
                'files': 0,
                'total_records': 0,
                'sample_data': []
            }
            
            if os.path.exists(data_dir):
                print(f"  ğŸ“ å¤„ç†ç›®å½•: {data_dir}")
                csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
                data_summary[hole_id]['files'] = len(csv_files)
                
                for csv_file in csv_files:
                    csv_path = os.path.join(data_dir, csv_file)
                    print(f"    ğŸ“‹ å¤„ç†æ–‡ä»¶: {csv_file}")
                    
                    # å°è¯•è¯»å–CSVæ–‡ä»¶
                    encodings = ['utf-8', 'gbk', 'gb2312']
                    for encoding in encodings:
                        try:
                            with open(csv_path, 'r', encoding=encoding) as f:
                                reader = csv.reader(f)
                                lines = list(reader)
                                
                            print(f"      ğŸ“Š æ•°æ®è¡Œæ•°: {len(lines)} (ç¼–ç : {encoding})")
                            data_summary[hole_id]['total_records'] += len(lines)
                            
                            if lines and len(lines) > 1:
                                # ä¿å­˜æ ·æœ¬æ•°æ®
                                header = lines[0]
                                sample_rows = lines[1:6]  # å‰5è¡Œæ•°æ®
                                
                                data_summary[hole_id]['sample_data'] = {
                                    'header': header,
                                    'rows': sample_rows
                                }
                                
                                print(f"      ğŸ“ è¡¨å¤´: {header[:5]}...")
                                print(f"      ğŸ“‹ æ ·æœ¬æ•°æ®: {len(sample_rows)}è¡Œ")
                            
                            break  # æˆåŠŸè¯»å–ï¼Œè·³å‡ºç¼–ç å¾ªç¯
                            
                        except UnicodeDecodeError:
                            continue
                        except Exception as e:
                            print(f"      âŒ è¯»å–å¤±è´¥ ({encoding}): {e}")
                            break
            else:
                print(f"  âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        
        # è¾“å‡ºæ±‡æ€»
        print("ğŸ“Š CSVæ•°æ®å¤„ç†æ±‡æ€»:")
        for hole_id, summary in data_summary.items():
            print(f"  {hole_id}:")
            print(f"    æ–‡ä»¶æ•°: {summary['files']}")
            print(f"    æ€»è®°å½•æ•°: {summary['total_records']}")
            if summary['sample_data']:
                print(f"    è¡¨å¤´å­—æ®µæ•°: {len(summary['sample_data']['header'])}")
        
        return True, data_summary
        
    except Exception as e:
        print(f"âŒ CSVæ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, {}

def test_data_integration():
    """æµ‹è¯•æ•°æ®é›†æˆåŠŸèƒ½"""
    print("ğŸ”— æµ‹è¯•æ•°æ®é›†æˆ...")
    
    try:
        # æ¨¡æ‹Ÿæ•°æ®é›†æˆé€»è¾‘
        print("ğŸ”§ æ¨¡æ‹Ÿå®æ—¶æ•°æ®æ¡¥æ¥...")
        
        # æ¨¡æ‹Ÿä»å¤šä¸ªæ¥æºåŠ è½½æ•°æ®
        sources = {
            'database': [],
            'filesystem': [],
            'cache': []
        }
        
        # æ¨¡æ‹Ÿæ•°æ®åº“æ•°æ®
        for i in range(10):
            sources['database'].append({
                'timestamp': datetime.now().isoformat(),
                'depth': i * 2.0,
                'diameter': 17.6 + np.random.normal(0, 0.01),
                'source': 'database',
                'operator': f'DB_User_{i%3}'
            })
        
        # æ¨¡æ‹Ÿæ–‡ä»¶ç³»ç»Ÿæ•°æ®
        for i in range(8):
            sources['filesystem'].append({
                'timestamp': datetime.now().isoformat(),
                'depth': i * 2.5,
                'diameter': 17.6 + np.random.normal(0, 0.01),
                'source': 'filesystem',
                'file': f'measurement_{i}.csv'
            })
        
        # æ¨¡æ‹Ÿç¼“å­˜æ•°æ®
        for i in range(5):
            sources['cache'].append({
                'timestamp': datetime.now().isoformat(),
                'depth': i * 3.0,
                'diameter': 17.6 + np.random.normal(0, 0.01),
                'source': 'cache',
                'cache_key': f'cache_{i}'
            })
        
        # æ•°æ®åˆå¹¶
        print("ğŸ”„ åˆå¹¶å¤šæºæ•°æ®...")
        all_data = []
        for source_name, data_list in sources.items():
            all_data.extend(data_list)
            print(f"  ğŸ“Š {source_name}: {len(data_list)}æ¡è®°å½•")
        
        # æ•°æ®å»é‡ï¼ˆåŸºäºæ·±åº¦ï¼‰
        print("ğŸ§¹ æ•°æ®å»é‡...")
        unique_data = {}
        for record in all_data:
            depth_key = f"{record['depth']:.1f}"
            if depth_key not in unique_data:
                unique_data[depth_key] = record
        
        final_data = list(unique_data.values())
        print(f"  ğŸ“Š å»é‡å: {len(final_data)}æ¡è®°å½•")
        
        # æ•°æ®æ’åº
        final_data.sort(key=lambda x: x['depth'])
        
        # æ•°æ®åˆ†æ
        print("ğŸ“ˆ æ•°æ®åˆ†æ:")
        depths = [d['depth'] for d in final_data]
        diameters = [d['diameter'] for d in final_data]
        
        print(f"  æ·±åº¦èŒƒå›´: {min(depths):.1f} - {max(depths):.1f}mm")
        print(f"  ç›´å¾„èŒƒå›´: {min(diameters):.3f} - {max(diameters):.3f}mm")
        print(f"  æ•°æ®æ¥æºåˆ†å¸ƒ:")
        
        source_count = {}
        for record in final_data:
            source = record['source']
            source_count[source] = source_count.get(source, 0) + 1
        
        for source, count in source_count.items():
            print(f"    {source}: {count}æ¡")
        
        return True, final_data
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, []

def generate_test_report(results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_results': {},
        'summary': {
            'total_tests': len(results),
            'passed_tests': sum(1 for r in results.values() if r['success']),
            'failed_tests': sum(1 for r in results.values() if not r['success'])
        }
    }
    
    for test_name, result in results.items():
        report['test_results'][test_name] = {
            'success': result['success'],
            'data_count': len(result.get('data', [])),
            'details': result.get('details', {})
        }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"history_data_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"  ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"  âŒ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨AIDCISå†å²æ•°æ®åç«¯åŠŸèƒ½æµ‹è¯•...")
    print("=" * 60)
    
    tests = [
        ("æ•°æ®åº“æ“ä½œ", test_database_operations),
        ("CSVæ•°æ®å¤„ç†", test_csv_data_processing),
        ("æ•°æ®é›†æˆ", test_data_integration),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        print("-" * 40)
        
        try:
            success, data = test_func()
            results[test_name] = {
                'success': success,
                'data': data,
                'details': {}
            }
            
            if success:
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
                
        except Exception as e:
            print(f"ğŸ’¥ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results[test_name] = {
                'success': False,
                'data': [],
                'details': {'error': str(e)}
            }
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "=" * 60)
    report = generate_test_report(results)
    
    # æ€»ç»“
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print("-" * 40)
    
    passed = report['summary']['passed_tests']
    total = report['summary']['total_tests']
    
    for test_name, result in results.items():
        status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
        data_count = len(result.get('data', []))
        print(f"  {test_name}: {status} (æ•°æ®: {data_count}æ¡)")
    
    print(f"\nğŸ¯ æ€»è®¡: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰å†å²æ•°æ®åç«¯åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ’¡ å†å²æ•°æ®åŠŸèƒ½å·²æˆåŠŸæ¿€æ´»ï¼")
        print("ğŸ”§ æ•°æ®å¤„ç†ç®¡é“è¿è¡Œæ­£å¸¸")
    else:
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return 0 if passed == total else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
